# =============================================================================
# SpiderFoot Microservices Docker Compose
# Decomposes SpiderFoot into individual services:
#   - api       : FastAPI REST API  (data layer, scan management)
#   - webui     : CherryPy web UI   (proxies all data through API)
#   - redis     : event bus + cache
#   - postgres  : persistent data store
#   - nginx     : reverse proxy / API gateway
#
# Usage:
#   cp docker/env.example .env         # customise settings
#   docker compose -f docker-compose-microservices.yml up --build -d
#
# Access:
#   Web UI  -> http://localhost                (via nginx)
#   API     -> http://localhost/api/docs       (Swagger UI)
#
# All services share the same Docker image built from /Dockerfile.
# The service role is selected via SF_SERVICE env var + command override.
# =============================================================================

# Shared build definition (anchored for YAML reuse)
x-sf-build: &sf-build
  build:
    context: .
    dockerfile: Dockerfile

networks:
  sf-frontend:
    driver: bridge
  sf-backend:
    driver: bridge
    internal: true

services:
  # ---------------------------------------------------------------------------
  # Infrastructure Services
  # ---------------------------------------------------------------------------

  # ---------------------------------------------------------------------------
  # MinIO — S3-compatible Object Storage
  # ---------------------------------------------------------------------------
  # Central persistent storage for all SpiderFoot data:
  #   - sf-logs       : Vector.dev log & event archive
  #   - sf-reports    : Generated scan reports (HTML, PDF, JSON, CSV)
  #   - sf-pg-backups : PostgreSQL WAL & pg_dump backups
  #   - sf-qdrant     : Qdrant vector DB snapshots
  #   - sf-data       : General application data / artefacts
  # ---------------------------------------------------------------------------

  minio:
    image: minio/minio:RELEASE.2024-11-07T00-52-20Z
    container_name: sf-minio
    restart: unless-stopped
    networks:
      - sf-backend
    ports:
      - "${SF_MINIO_API_PORT:-9000}:9000"
      - "${SF_MINIO_CONSOLE_PORT:-9001}:9001"
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER:-spiderfoot}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD:-changeme123}
    volumes:
      - minio-data:/data
    command: server /data --console-address ":9001"
    healthcheck:
      test: ["CMD", "mc", "ready", "local"]
      interval: 15s
      timeout: 5s
      retries: 5
      start_period: 10s
    deploy:
      resources:
        limits:
          cpus: "${MINIO_CPUS:-1.0}"
          memory: "${MINIO_MEMORY:-512M}"

  # MinIO bucket initializer — creates required buckets on first start
  # Also copies the mc binary to a shared volume for the pg-backup sidecar.
  minio-init:
    image: minio/mc:RELEASE.2024-11-17T19-35-25Z
    container_name: sf-minio-init
    networks:
      - sf-backend
    depends_on:
      minio:
        condition: service_healthy
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER:-spiderfoot}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD:-changeme123}
    volumes:
      - mc-bin:/opt/mc-share
    entrypoint: >
      /bin/sh -c "
        mc alias set sfminio http://minio:9000 $${MINIO_ROOT_USER} $${MINIO_ROOT_PASSWORD};
        mc mb --ignore-existing sfminio/sf-logs;
        mc mb --ignore-existing sfminio/sf-reports;
        mc mb --ignore-existing sfminio/sf-pg-backups;
        mc mb --ignore-existing sfminio/sf-qdrant-snapshots;
        mc mb --ignore-existing sfminio/sf-data;
        mc mb --ignore-existing sfminio/sf-loki-data;
        mc mb --ignore-existing sfminio/sf-loki-ruler;
        mc mb --ignore-existing sfminio/sf-enrichment;
        mc anonymous set download sfminio/sf-reports;
        cp /usr/bin/mc /opt/mc-share/mc && chmod +x /opt/mc-share/mc;
        echo 'MinIO buckets initialized + mc binary shared';
      "

  redis:
    image: redis:7-alpine
    container_name: sf-redis
    restart: unless-stopped
    networks:
      - sf-backend
    command: >
      redis-server
        --appendonly yes
        --maxmemory 512mb
        --maxmemory-policy allkeys-lru
    volumes:
      - redis-data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  postgres:
    image: postgres:15-alpine
    container_name: sf-postgres
    restart: unless-stopped
    networks:
      - sf-backend
    environment:
      POSTGRES_DB: ${POSTGRES_DB:-spiderfoot}
      POSTGRES_USER: ${POSTGRES_USER:-spiderfoot}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-changeme}
    volumes:
      - postgres-data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U spiderfoot"]
      interval: 10s
      timeout: 5s
      retries: 5

  # ---------------------------------------------------------------------------
  # SpiderFoot API Service (FastAPI)
  # ---------------------------------------------------------------------------

  api:
    <<: *sf-build
    image: spiderfoot-micro:latest
    container_name: sf-api
    restart: unless-stopped
    networks:
      - sf-frontend
      - sf-backend
    environment:
      SF_SERVICE: api
      SF_SERVICE_ROLE: api
      SF_DEPLOYMENT_MODE: microservice
      SF_REDIS_URL: redis://redis:6379/0
      SF_POSTGRES_DSN: postgresql://${POSTGRES_USER:-spiderfoot}:${POSTGRES_PASSWORD:-changeme}@postgres:5432/${POSTGRES_DB:-spiderfoot}
      SF_API_HOST: "0.0.0.0"
      SF_API_PORT: "8001"
      SF_API_WORKERS: ${SF_API_WORKERS:-2}
      SF_LOG_LEVEL: ${SF_LOG_LEVEL:-INFO}
      SF_API_KEY: ${SF_API_KEY:-}
      SF_VECTOR_URL: "http://vector:${SF_VECTOR_HTTP_PORT:-8686}"
      # Qdrant vector DB settings
      SF_QDRANT_BACKEND: "http"
      SF_QDRANT_HOST: "qdrant"
      SF_QDRANT_PORT: "6333"
      SF_QDRANT_GRPC_PORT: "6334"
      SF_QDRANT_PREFIX: "sf_"
      # Embedding model settings
      SF_EMBEDDING_PROVIDER: ${SF_EMBEDDING_PROVIDER:-mock}
      SF_EMBEDDING_MODEL: ${SF_EMBEDDING_MODEL:-all-MiniLM-L6-v2}
      SF_EMBEDDING_DIMENSIONS: ${SF_EMBEDDING_DIMENSIONS:-384}
      SF_EMBEDDING_API_KEY: ${SF_EMBEDDING_API_KEY:-}
      SF_EMBEDDING_API_BASE: ${SF_EMBEDDING_API_BASE:-}
      # Reranker settings
      SF_RERANKER_PROVIDER: ${SF_RERANKER_PROVIDER:-mock}
      SF_RERANKER_MODEL: ${SF_RERANKER_MODEL:-cross-encoder/ms-marco-MiniLM-L-6-v2}
      SF_RERANKER_API_KEY: ${SF_RERANKER_API_KEY:-}
      SF_RERANKER_TOP_K: ${SF_RERANKER_TOP_K:-10}
      SF_RERANKER_SCORE_THRESHOLD: ${SF_RERANKER_SCORE_THRESHOLD:-0.3}
      # RAG / LLM settings (routed through LiteLLM proxy)
      SF_LLM_PROVIDER: ${SF_LLM_PROVIDER:-mock}
      SF_LLM_MODEL: ${SF_LLM_MODEL:-}
      SF_LLM_API_KEY: ${SF_LLM_API_KEY:-sk-sf-litellm-dev-key}
      SF_LLM_API_BASE: ${SF_LLM_API_BASE:-http://litellm:4000}
      # Vector correlation settings
      SF_VECTOR_CORRELATION_ENABLED: ${SF_VECTOR_CORRELATION_ENABLED:-true}
      SF_VECTOR_SIMILARITY_THRESHOLD: ${SF_VECTOR_SIMILARITY_THRESHOLD:-0.7}
      SF_VECTOR_CROSS_SCAN_THRESHOLD: ${SF_VECTOR_CROSS_SCAN_THRESHOLD:-0.8}
      SF_VECTOR_RERANK_ACCURACY: ${SF_VECTOR_RERANK_ACCURACY:-high}
      # MinIO / S3-compatible object storage
      SF_MINIO_ENDPOINT: "minio:9000"
      SF_MINIO_ACCESS_KEY: ${MINIO_ROOT_USER:-spiderfoot}
      SF_MINIO_SECRET_KEY: ${MINIO_ROOT_PASSWORD:-changeme123}
      SF_MINIO_SECURE: "false"
      SF_MINIO_REPORTS_BUCKET: ${SF_MINIO_REPORTS_BUCKET:-sf-reports}
      SF_MINIO_DATA_BUCKET: ${SF_MINIO_DATA_BUCKET:-sf-data}
      SF_MINIO_LOGS_BUCKET: ${SF_MINIO_LOGS_BUCKET:-sf-logs}
    depends_on:
      redis:
        condition: service_healthy
      postgres:
        condition: service_healthy
      qdrant:
        condition: service_healthy
      minio:
        condition: service_healthy
    entrypoint: ["/usr/local/bin/docker-entrypoint.sh"]
    command: ["python", "-m", "spiderfoot.service_runner", "--service", "api", "--port", "8001"]
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8001/api/docs')"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: "${API_CPUS:-2.0}"
          memory: "${API_MEMORY:-2G}"

  # ---------------------------------------------------------------------------
  # SpiderFoot Web UI Service (CherryPy)
  # ---------------------------------------------------------------------------

  webui:
    <<: *sf-build
    image: spiderfoot-micro:latest
    container_name: sf-webui
    restart: unless-stopped
    networks:
      - sf-frontend
      - sf-backend
    environment:
      SF_SERVICE: webui
      SF_SERVICE_ROLE: webui
      SF_DEPLOYMENT_MODE: microservice
      SF_WEB_HOST: "0.0.0.0"
      SF_WEB_PORT: "5001"
      SF_LOG_LEVEL: ${SF_LOG_LEVEL:-INFO}
      SF_WEBUI_API_MODE: "true"
      SF_WEBUI_API_URL: "http://api:8001/api"
      SF_WEBUI_API_KEY: ${SF_API_KEY:-}
      SF_POSTGRES_DSN: postgresql://${POSTGRES_USER:-spiderfoot}:${POSTGRES_PASSWORD:-changeme}@postgres:5432/${POSTGRES_DB:-spiderfoot}
    depends_on:
      api:
        condition: service_healthy
    entrypoint: ["/usr/local/bin/docker-entrypoint.sh"]
    command: ["python", "-m", "spiderfoot.service_runner", "--service", "webui", "--port", "5001"]
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:5001/')"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: "${WEBUI_CPUS:-1.0}"
          memory: "${WEBUI_MEMORY:-1G}"

  # ---------------------------------------------------------------------------
  # Nginx Reverse Proxy / API Gateway
  # ---------------------------------------------------------------------------

  nginx:
    image: nginx:1.25-alpine
    container_name: sf-nginx
    restart: unless-stopped
    networks:
      - sf-frontend
      - sf-backend
    ports:
      - "${SF_HTTP_PORT:-80}:80"
    volumes:
      - ./docker/nginx-microservices.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      api:
        condition: service_healthy
      webui:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://127.0.0.1:80/nginx-health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # ---------------------------------------------------------------------------
  # Qdrant — Vector Database for RAG / Rerank Correlations
  # ---------------------------------------------------------------------------
  # Provides per-scan and per-workspace vector collections for semantic
  # similarity search, cross-scan correlation, and multi-hop analysis.
  # Events are embedded and indexed during scans; workspace meta-collections
  # aggregate multiple scan collections for cross-scan discovery.
  # ---------------------------------------------------------------------------

  qdrant:
    image: qdrant/qdrant:v1.12.4
    container_name: sf-qdrant
    restart: unless-stopped
    networks:
      - sf-backend
    volumes:
      - qdrant-data:/qdrant/storage
    environment:
      QDRANT__SERVICE__GRPC_PORT: "6334"
      QDRANT__SERVICE__HTTP_PORT: "6333"
      # S3 snapshot storage via MinIO
      QDRANT__STORAGE__SNAPSHOTS_PATH: "/qdrant/snapshots"
    depends_on:
      minio:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "bash -c 'echo > /dev/tcp/localhost/6333'"]
      interval: 15s
      timeout: 5s
      retries: 5
      start_period: 10s
    deploy:
      resources:
        limits:
          cpus: "${QDRANT_CPUS:-1.0}"
          memory: "${QDRANT_MEMORY:-1G}"

  # ---------------------------------------------------------------------------
  # Vector.dev — Data Pipeline (logs, events, metrics)
  # ---------------------------------------------------------------------------
  # Integration point for shipping scan data to external storage/analytics:
  #   - Loki: centralized log aggregation
  #   - Prometheus: metrics via exporter
  #   - S3/Minio: long-term archival of all scan events
  #   - Webhooks: real-time high-severity alerts
  #
  # Vector.dev replaces Promtail + OTel Collector, serving as the unified
  # telemetry pipeline (logs, metrics, traces).
  #
  # Enable sinks via env vars — see config/vector.toml for details.
  # ---------------------------------------------------------------------------

  vector:
    image: timberio/vector:0.53.0-alpine
    container_name: sf-vector
    restart: unless-stopped
    networks:
      - sf-backend
    ports:
      - "9598:9598"    # Prometheus metrics exporter
      - "4317:4317"    # OTLP gRPC (trace ingestion)
      - "4318:4318"    # OTLP HTTP (trace ingestion)
    volumes:
      - ./config/vector.toml:/etc/vector/vector.toml:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - vector-data:/var/lib/vector
      - vector-logs:/var/log/vector
    environment:
      VECTOR_CONFIG: /etc/vector/vector.toml
      SF_VECTOR_HTTP_PORT: ${SF_VECTOR_HTTP_PORT:-8686}
      SF_ES_ENDPOINT: ${SF_ES_ENDPOINT:-http://elasticsearch:9200}
      SF_LOKI_ENDPOINT: ${SF_LOKI_ENDPOINT:-http://loki:3100}
      SF_ALERT_WEBHOOK_URL: ${SF_ALERT_WEBHOOK_URL:-http://localhost:9999/alert}
      SF_S3_BUCKET: ${SF_S3_BUCKET:-spiderfoot-data-archive}
      SF_S3_REGION: ${SF_S3_REGION:-us-east-1}
      # MinIO S3-compatible endpoint for log/event archival
      AWS_ACCESS_KEY_ID: ${MINIO_ROOT_USER:-spiderfoot}
      AWS_SECRET_ACCESS_KEY: ${MINIO_ROOT_PASSWORD:-changeme123}
      SF_MINIO_ENDPOINT: "http://minio:9000"
      SF_MINIO_LOGS_BUCKET: ${SF_MINIO_LOGS_BUCKET:-sf-logs}
    depends_on:
      api:
        condition: service_healthy
      minio:
        condition: service_healthy
      loki:
        condition: service_started
      jaeger:
        condition: service_started
    deploy:
      resources:
        limits:
          cpus: "${VECTOR_CPUS:-0.5}"
          memory: "${VECTOR_MEMORY:-256M}"

  # ---------------------------------------------------------------------------
  # Loki — Log Aggregation (Grafana-native)
  # ---------------------------------------------------------------------------
  # Receives logs from Vector.dev (replaces Promtail).  Uses MinIO for
  # chunk/index storage so logs survive container restarts.
  # ---------------------------------------------------------------------------

  loki:
    image: grafana/loki:3.3.2
    container_name: sf-loki
    restart: unless-stopped
    networks:
      - sf-backend
    volumes:
      - ./infra/loki/local-config.yaml:/etc/loki/local-config.yaml:ro
    command: -config.file=/etc/loki/local-config.yaml
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER:-spiderfoot}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD:-changeme123}
    depends_on:
      minio:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "wget -q --spider http://localhost:3100/ready || exit 1"]
      interval: 15s
      timeout: 5s
      retries: 5
      start_period: 20s
    deploy:
      resources:
        limits:
          cpus: "${LOKI_CPUS:-1.0}"
          memory: "${LOKI_MEMORY:-512M}"

  # ---------------------------------------------------------------------------
  # Grafana — Dashboards & Observability UI
  # ---------------------------------------------------------------------------
  # Pre-provisioned with datasources (Loki, Prometheus, PostgreSQL) and
  # dashboards for scan monitoring, event analytics, and service health.
  # ---------------------------------------------------------------------------

  grafana:
    image: grafana/grafana:11.4.0
    container_name: sf-grafana
    restart: unless-stopped
    networks:
      - sf-frontend
      - sf-backend
    ports:
      - "${SF_GRAFANA_PORT:-3000}:3000"
    environment:
      GF_SECURITY_ADMIN_USER: ${GF_SECURITY_ADMIN_USER:-admin}
      GF_SECURITY_ADMIN_PASSWORD: ${GF_SECURITY_ADMIN_PASSWORD:-spiderfoot}
      GF_SERVER_ROOT_URL: ${GF_SERVER_ROOT_URL:-http://localhost:3000}
      GF_SERVER_SERVE_FROM_SUB_PATH: "false"
      GF_AUTH_ANONYMOUS_ENABLED: "true"
      GF_AUTH_ANONYMOUS_ORG_ROLE: "Viewer"
      GF_USERS_ALLOW_SIGN_UP: "false"
      GF_INSTALL_PLUGINS: "grafana-piechart-panel"
      # Database connection for PostgreSQL datasource
      SF_POSTGRES_DSN: postgresql://${POSTGRES_USER:-spiderfoot}:${POSTGRES_PASSWORD:-changeme}@postgres:5432/${POSTGRES_DB:-spiderfoot}
    volumes:
      - ./infra/grafana/provisioning:/etc/grafana/provisioning:ro
      - ./infra/grafana/dashboards:/var/lib/grafana/dashboards:ro
      - grafana-data:/var/lib/grafana
    depends_on:
      loki:
        condition: service_healthy
      prometheus:
        condition: service_started
    healthcheck:
      test: ["CMD-SHELL", "wget -q --spider http://localhost:3000/api/health || exit 1"]
      interval: 15s
      timeout: 5s
      retries: 5
      start_period: 20s
    deploy:
      resources:
        limits:
          cpus: "${GRAFANA_CPUS:-1.0}"
          memory: "${GRAFANA_MEMORY:-512M}"

  # ---------------------------------------------------------------------------
  # Prometheus — Metrics Collection & Storage
  # ---------------------------------------------------------------------------
  # Scrapes metrics from SpiderFoot API, Vector.dev, Qdrant, MinIO, and
  # future services (agents, enrichment, LiteLLM, Jaeger).
  # ---------------------------------------------------------------------------

  prometheus:
    image: prom/prometheus:v2.54.1
    container_name: sf-prometheus
    restart: unless-stopped
    networks:
      - sf-backend
    ports:
      - "${SF_PROMETHEUS_PORT:-9090}:9090"
    volumes:
      - ./infra/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=30d'
      - '--web.enable-lifecycle'
    healthcheck:
      test: ["CMD-SHELL", "wget -q --spider http://localhost:9090/-/healthy || exit 1"]
      interval: 15s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: "${PROMETHEUS_CPUS:-1.0}"
          memory: "${PROMETHEUS_MEMORY:-512M}"

  # ---------------------------------------------------------------------------
  # SpiderFoot User Input Service
  # ---------------------------------------------------------------------------
  # API for user-supplied investigation data:
  #   - Document/report uploads for enrichment
  #   - IOC list submissions for cross-referencing
  #   - Context data to enhance scan analysis
  #   - Batch target lists for multi-target scanning
  # ---------------------------------------------------------------------------

  user-input:
    <<: *sf-build
    image: spiderfoot-micro:latest
    container_name: sf-user-input
    restart: unless-stopped
    networks:
      - sf-frontend
      - sf-backend
    ports:
      - "${SF_INPUT_PORT:-8300}:8300"
    environment:
      SF_SERVICE: user-input
      SF_INPUT_HOST: "0.0.0.0"
      SF_INPUT_PORT: "8300"
      SF_LOG_LEVEL: ${SF_LOG_LEVEL:-INFO}
      # Internal service URLs
      SF_ENRICHMENT_URL: "http://enrichment:8200"
      SF_AGENTS_URL: "http://agents:8100"
      # MinIO storage
      SF_MINIO_ENDPOINT: "minio:9000"
      SF_MINIO_ACCESS_KEY: ${MINIO_ROOT_USER:-spiderfoot}
      SF_MINIO_SECRET_KEY: ${MINIO_ROOT_PASSWORD:-changeme123}
      # OTEL tracing
      SF_OTEL_ENDPOINT: "http://vector:4317"
      SF_SERVICE_NAME: "spiderfoot-user-input"
    entrypoint: ["/usr/local/bin/docker-entrypoint.sh"]
    command: ["python", "-m", "spiderfoot.user_input.service"]
    depends_on:
      enrichment:
        condition: service_healthy
      agents:
        condition: service_healthy
      minio:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8300/health')"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 20s
    deploy:
      resources:
        limits:
          cpus: "${INPUT_CPUS:-1.0}"
          memory: "${INPUT_MEMORY:-1G}"

  # ---------------------------------------------------------------------------
  # SpiderFoot Enrichment Service (Document Processing Pipeline)
  # ---------------------------------------------------------------------------
  # Processes uploaded documents and scan artifacts through:
  #   - Text extraction (PDF, DOCX, XLSX, HTML, etc.)
  #   - Entity/IOC extraction (IPs, domains, hashes, CVEs, etc.)
  #   - MinIO storage for original + extracted content
  #   - Dispatch to agents for LLM-powered analysis
  # ---------------------------------------------------------------------------

  enrichment:
    <<: *sf-build
    image: spiderfoot-micro:latest
    container_name: sf-enrichment
    restart: unless-stopped
    networks:
      - sf-frontend
      - sf-backend
    ports:
      - "${SF_ENRICHMENT_PORT:-8200}:8200"
    environment:
      SF_SERVICE: enrichment
      SF_ENRICHMENT_HOST: "0.0.0.0"
      SF_ENRICHMENT_PORT: "8200"
      SF_ENRICHMENT_WORKERS: ${SF_ENRICHMENT_WORKERS:-2}
      SF_LOG_LEVEL: ${SF_LOG_LEVEL:-INFO}
      # MinIO storage
      SF_MINIO_ENDPOINT: "minio:9000"
      SF_MINIO_ACCESS_KEY: ${MINIO_ROOT_USER:-spiderfoot}
      SF_MINIO_SECRET_KEY: ${MINIO_ROOT_PASSWORD:-changeme123}
      # Optional Apache Tika for complex document formats
      SF_TIKA_ENDPOINT: ${SF_TIKA_ENDPOINT:-}
      # OTEL tracing
      SF_OTEL_ENDPOINT: "http://vector:4317"
      SF_SERVICE_NAME: "spiderfoot-enrichment"
    entrypoint: ["/usr/local/bin/docker-entrypoint.sh"]
    command: ["python", "-m", "spiderfoot.enrichment.service"]
    depends_on:
      minio:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8200/health')"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 20s
    deploy:
      resources:
        limits:
          cpus: "${ENRICHMENT_CPUS:-2.0}"
          memory: "${ENRICHMENT_MEMORY:-2G}"

  # ---------------------------------------------------------------------------
  # SpiderFoot Agents Service (AI analysis agents)
  # ---------------------------------------------------------------------------
  # Hosts AI-powered analysis agents that process scan events:
  #   - FindingValidator: Validates high-risk findings via LLM
  #   - CredentialAnalyzer: Assesses exposed credential risk
  #   - TextSummarizer: Summarizes large content for intelligence
  #   - ReportGenerator: Generates comprehensive scan reports
  #   - DocumentAnalyzer: Analyzes user-uploaded documents
  #   - ThreatIntelAnalyzer: Cross-references findings with threat intel
  # ---------------------------------------------------------------------------

  agents:
    <<: *sf-build
    image: spiderfoot-micro:latest
    container_name: sf-agents
    restart: unless-stopped
    networks:
      - sf-frontend
      - sf-backend
    ports:
      - "${SF_AGENTS_PORT:-8100}:8100"
    environment:
      SF_SERVICE: agents
      SF_AGENTS_HOST: "0.0.0.0"
      SF_AGENTS_PORT: "8100"
      SF_AGENTS_WORKERS: ${SF_AGENTS_WORKERS:-1}
      SF_LOG_LEVEL: ${SF_LOG_LEVEL:-INFO}
      SF_REDIS_URL: redis://redis:6379/0
      # LLM via LiteLLM proxy
      SF_LLM_PROVIDER: ${SF_LLM_PROVIDER:-mock}
      SF_LLM_API_BASE: ${SF_LLM_API_BASE:-http://litellm:4000}
      SF_LLM_API_KEY: ${SF_LLM_API_KEY:-sk-sf-litellm-dev-key}
      SF_LLM_MODEL: ${SF_LLM_MODEL:-gpt-4o-mini}
      # OTEL tracing
      SF_OTEL_ENDPOINT: "http://vector:4317"
      SF_SERVICE_NAME: "spiderfoot-agents"
    entrypoint: ["/usr/local/bin/docker-entrypoint.sh"]
    command: ["python", "-m", "spiderfoot.agents.service"]
    depends_on:
      redis:
        condition: service_healthy
      litellm:
        condition: service_started
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8100/health')"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: "${AGENTS_CPUS:-2.0}"
          memory: "${AGENTS_MEMORY:-2G}"

  # ---------------------------------------------------------------------------
  # LiteLLM Proxy — Unified LLM Gateway
  # ---------------------------------------------------------------------------
  # Routes all LLM requests through a single API endpoint with:
  #   - Multi-provider support (OpenAI, Anthropic, Ollama, Azure)
  #   - Cost tracking & usage analytics
  #   - Redis-backed response caching
  #   - Prometheus metrics for LLM operations
  #   - Model fallback chains
  # All SpiderFoot services connect to litellm:4000 instead of
  # individual LLM providers.
  # ---------------------------------------------------------------------------

  litellm:
    image: ghcr.io/berriai/litellm:main-latest
    container_name: sf-litellm
    restart: unless-stopped
    networks:
      - sf-frontend
      - sf-backend
    ports:
      - "${SF_LITELLM_PORT:-4000}:4000"
    volumes:
      - ./infra/litellm/config.yaml:/app/config.yaml:ro
    environment:
      LITELLM_MASTER_KEY: ${LITELLM_MASTER_KEY:-sk-sf-litellm-dev-key}
      LITELLM_DATABASE_URL: postgresql://${POSTGRES_USER:-spiderfoot}:${POSTGRES_PASSWORD:-changeme}@postgres:5432/${POSTGRES_DB:-spiderfoot}
      OPENAI_API_KEY: ${OPENAI_API_KEY:-}
      ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY:-}
      OLLAMA_API_BASE: ${OLLAMA_API_BASE:-http://host.docker.internal:11434}
      # OTEL tracing to Vector
      OTEL_EXPORTER_OTLP_ENDPOINT: "http://vector:4317"
      OTEL_SERVICE_NAME: "litellm"
    command: ["--config", "/app/config.yaml", "--port", "4000", "--num_workers", "2"]
    depends_on:
      redis:
        condition: service_healthy
      postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "wget -q --spider http://localhost:4000/health || exit 1"]
      interval: 15s
      timeout: 5s
      retries: 5
      start_period: 20s
    deploy:
      resources:
        limits:
          cpus: "${LITELLM_CPUS:-1.0}"
          memory: "${LITELLM_MEMORY:-512M}"

  # ---------------------------------------------------------------------------
  # Jaeger — Distributed Tracing
  # ---------------------------------------------------------------------------
  # All-in-one Jaeger instance for trace collection and visualization.
  # Services send traces via OTLP (gRPC/HTTP) through Vector.dev or directly.
  # UI accessible via nginx /jaeger/ or direct port 16686.
  # ---------------------------------------------------------------------------

  jaeger:
    image: jaegertracing/jaeger:2.4.0
    container_name: sf-jaeger
    restart: unless-stopped
    networks:
      - sf-frontend
      - sf-backend
    ports:
      - "${SF_JAEGER_UI_PORT:-16686}:16686"    # Jaeger UI
    environment:
      COLLECTOR_OTLP_ENABLED: "true"
    healthcheck:
      test: ["CMD-SHELL", "wget -q --spider http://localhost:13133/status || exit 1"]
      interval: 15s
      timeout: 5s
      retries: 5
      start_period: 10s
    deploy:
      resources:
        limits:
          cpus: "${JAEGER_CPUS:-1.0}"
          memory: "${JAEGER_MEMORY:-512M}"

  # ---------------------------------------------------------------------------
  # PostgreSQL Backup Sidecar — pg_dump to MinIO
  # ---------------------------------------------------------------------------
  # Runs pg_dump on a schedule and uploads to MinIO sf-pg-backups bucket.
  # Default: every 6 hours. Override with SF_PG_BACKUP_SCHEDULE.
  # ---------------------------------------------------------------------------

  pg-backup:
    image: postgres:15-alpine
    container_name: sf-pg-backup
    restart: unless-stopped
    networks:
      - sf-backend
    environment:
      PGHOST: postgres
      PGPORT: "5432"
      PGUSER: ${POSTGRES_USER:-spiderfoot}
      PGPASSWORD: ${POSTGRES_PASSWORD:-changeme}
      PGDATABASE: ${POSTGRES_DB:-spiderfoot}
      MINIO_ENDPOINT: "http://minio:9000"
      MINIO_ACCESS_KEY: ${MINIO_ROOT_USER:-spiderfoot}
      MINIO_SECRET_KEY: ${MINIO_ROOT_PASSWORD:-changeme123}
      BACKUP_BUCKET: sf-pg-backups
      BACKUP_SCHEDULE_HOURS: ${SF_PG_BACKUP_SCHEDULE:-6}
      BACKUP_RETENTION_DAYS: ${SF_PG_BACKUP_RETENTION:-30}
    depends_on:
      postgres:
        condition: service_healthy
      minio:
        condition: service_healthy
      minio-init:
        condition: service_completed_successfully
    volumes:
      - ./scripts/pg_backup_minio.sh:/usr/local/bin/pg_backup_minio.sh:ro
      - mc-bin:/opt/mc-share:ro
    entrypoint: ["/bin/sh", "/usr/local/bin/pg_backup_minio.sh"]
    deploy:
      resources:
        limits:
          cpus: "0.25"
          memory: "128M"

volumes:
  redis-data:
  postgres-data:
  qdrant-data:
  vector-data:
  vector-logs:
  minio-data:
  mc-bin:
  grafana-data:
  prometheus-data:
